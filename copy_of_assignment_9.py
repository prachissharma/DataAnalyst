# -*- coding: utf-8 -*-
"""Copy of assignment_9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zzTXB_NyyoUcWskFZ7r5-ox9ZNJUez8o

# ![INTTRVU_logo_Jupyter_notebook.jpg](attachment:dee5cac9-50a6-4ea6-adc3-c6d42d70f7eb.jpg)
"""



# Question 1
# Explain how KNN works for classfication?

### KNN (K-Nearest Neighbors) is a supervised machine learning algorithm that classifies a new data point based on how similar it is to existing labeled data.For a new data point, compute the distance between this point and all the existing training points using metrics like:Euclidean distance (most common)Manhattan distance, Minkowski distance. here we choose The class with the highest count (majority vote) becomes the predicted class for the new point.



# Question 2
# Explain how KNN works for regression?

### as we know that regression will work for numerical data so, KNN Regression predicts a numerical value (not a class).For the new data point, compute the distance to all training points using:Euclidean distance (most common),Manhattan distance,Minkowski distance. same here we will use to Pick the K points with the smallest distance to the new point.For regression:Look at the numerical target values of these K neighbors.Compute the mean of those values.here in this method calculated average will become the new data point.



# Question 3
# How can we decide value of K in KNN?
### through the square root we  can get value of K :Use the Square Root Rule (Heuristic)A common simple rule is: k=root of value of N , Where N = number of data points.Example:If you have 400 samples →then value of square root of 4oo will be 20 .so, here in example 20 will gives a good starting point.





# Question 4
# Do we require feature scaling in KNN? If yes, why?
### yes, feature scalling is mendatory in KNN method so for that feature scaling is required in KNN because the algorithm uses distance-based calculations. If features are on different scales, larger-valued features will dominate the distance measure, leading to incorrect neighbors and poor model performance. Scaling ensures all features contribute equally. for an example: Suppose you have two features:Age: ranges from 20 to 60,Income: ranges from 20,000 to 2,00,000,Income has a much larger numerical range, so it will dominate the distance calculation. KNN will mostly consider income and ignore age, even if age is more important for classification.This makes the model inaccurate.in this case we will use standardised method or normalization (min -max scalling) in KNN algorithm





# Question 5
# What is weighted KNN?

###answer : Weighted KNN assigns weights to the K nearest neighbors based on their distance — closer points get higher weight, farther points get lower weight. The final prediction is calculated using these weighted contributions. for an example Example:Neighbor votes:Class A with weight 0.50,Class A with weight 0.40,Class B with weight 0.10,Result: Class A wins.Closer neighbors influence the prediction more strongly.according to this class a has a good influence



# Question 6
# How can we use KNN for anomaly detection?
### KNN is a distance-based method, so it can be used to detect anomalies (outliers) as points that are far away from their neighbors.Anomaly score can be:Distance to the Kth nearest neighbor,Average distance to the K nearest neighbors,Points with large distances are likely outliers, because they are far from the normal data cluster.Choose a threshold distance above which points are considered anomalies.Points below the threshold → normal ,Points above the threshold → anomalies





# Questions 7
# SVM (Support Vector Machine) is a supervised learning algorithm that finds the best boundary to separate data points of different classes.
### ANSWER: The Maximum Margin Hyperplane is the decision boundary that:Separates the classes as cleanly as possible, Maximizes the distance (margin) between the boundary and the nearest points of each class:A hyperplane is a line (2D), plane (3D), or subspace (higher dimensions) that separates the classes.Equation of hyperplane (2D):W*X+B=0, WHERE where,w = weight vector,b = bias, x = input features. Margin = distance between the hyperplane and the nearest data point from each class.Support Vectors,The data points that lie closest to the hyperplane.They define the margin.Only these points influence the position of the hyperplane.Maximizing the margin reduces generalization error,Makes the model more robust to new, unseen data





# Question 8
# What is hinge loss in SVM? Why it is useful?

### Hinge loss is the loss function used by Support Vector Machines (SVM) for classification.It measures how well the predicted class matches the true class and penalizes misclassified points.If the point is correctly classified and far enough from the hyperplane (margin ≥ 1):Hinge loss = 0 → no penaltyIf the point is on the wrong side or inside the margin:Hinge loss > 0 → penalty proportional to distance from correct margin.Encourages large margin:Hinge loss penalizes points inside the margin even if correctly classified, helping maximize the margin.Focus on misclassified points:Only points that are misclassified or too close to the boundary contribute to the loss.Supports robust classification:Leads to better generalization on unseen data.



# Question 9
# Can we use SVM for building non-linear classfier? How?
###A standard SVM finds a linear hyperplane to separate classes. But many real-world datasets are not linearly separable. To handle this, SVM uses kernels to map data into a higher-dimensional space where it becomes linearly separable.Map data to higher dimensions. Use a function φ(x) to transform original features into a higher-dimensional space.In this space, a linear hyperplane can separate the classes. Use kernel function .Instead of computing φ(x) explicitly, SVM uses a kernel function K(x, x') to calculate inner products in high-dimensional space efficiently. This is called the kernel trick.





# Question 10
# What is significance of C in SVM optimization problem? What would happen if value of C is very high?
### ANSWER : In SVM (Support Vector Machine), C is one of the most important hyperparameters. It controls how much you want to penalize misclassification errors during training.C controls the trade-off between margin width and classification errors. WHEN C IS LOW : Then,Model allows more misclassifications Margin becomes wider SVM becomes less strict .More generalization (higher bias, lower variance)  and If C is High then , Model tries to avoid any misclassification Margin becomes very narrow SVM becomes strict and sensitive to noise ,Risk of overfitting increases (low bias, high variance).If C is extremely high, SVM behaves like: “Classify every point correctly, even if the margin becomes tiny.” This makes the model very sensitive to outliers and noise.





# Question 11
# What is the need of dimesion reduction algorithm?

###Dimension reduction algorithms are needed because real-world data often has too many features, which can create problems for machine learning models. Reducing dimensions helps simplify the data while keeping the important information.1.o Reduce Overfitting High-dimensional data makes models memorize noise. Reducing features helps the model generalize better.2.To Improve Model Performance Fewer features → faster training → lower computation cost.Algorithms like KNN, SVM, and clustering perform better after reducing dimensions.3. To Remove Irrelevant or Correlated Features Many features don’t add value. Some features carry duplicate information (high correlation).Dimensionality reduction extracts the most meaningful components. To Solve the Curse of Dimensionality As dimensions increase, distance metrics become less meaningful. Clustering and KNN fail in very high-dimensional space. Reducing dimensions makes these algorithms effective again.To Visualize High-Dimensional DataTechniques like PCA, t-SNE, UMAP reduce data to 2D or 3D for visualization.Helps detect clusters, patterns, and anomalies.



# Question 12
# What are the steps in dimension reduction using PCA?
### PCA affected by Scalling ( 0 to 1 ) and PCA is a mathematical method that transforms high-dimensional data into a smaller set of new features (called principal components) that capture the maximum variance in the data.so, hencefor, we used Compute Eigenvalues and Eigenvectors ,Eigenvectors → directions of maximum variance (principal components). Eigenvalues → how much variance each component captures.  if Higher eigenvalue = more important component. Sort eigenvectors according to these values. Choose K components based on: Variance explained (e.g., 95%) ,Scree plot ,Cumulative variance. PCA is used for To remove noise and redundant features To speed up machine learning algorithmsTo avoid overfitting ,To visualize high-dimensional data in 2D or 3D.





# Question 13

# How can we decide number of dimensions to be choosed from output of PCA?
### answer : throgh cumulative  explained variance, Example: If cumulative variance is: PC1 → 70% ,PC1+PC2 → 92% ,PC1+PC2+PC3 → 96% ,You choose PC1 + PC2 (since 92% ≥ 90% or 95%).A Scree plot shows eigenvalues vs. number of components.Rule: Look for the elbow point → the point after which the curve flattens. Components before the elbow are meaningful. This is called the elbow method. another method is Kaiser’s Criterion (For PCA on Covariance Matrix)Keep components whose eigenvalue > 1 , Indicates components with above-average information. through cross validation : Train your ML model on increasing number of PCs.Choose the number of PCs where model accuracy stops improving.through these above methods we can calculate  the optimal PCA dimensions .





# Question 14 - ML Model Programming
# Build a model to classify breast tumors as malignant or benign using cell nucleus features .PCA will be applied to reduce dimensionality.
# -------------------------------------------------------------
# Breast Cancer Classification with PCA
# -------------------------------------------------------------

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# -------------------------------------------------------------
# 1. Load Dataset
# -------------------------------------------------------------
data_path = "/content/sample_data/Cancer Dataset.csv"
df = pd.read_csv(data_path)

print("Dataset Loaded Successfully!")
print(df.head())
# Remove the unwanted column with all NaN values
if "Unnamed: 32" in df.columns:
    df = df.drop("Unnamed: 32", axis=1)

# Drop any remaining rows with NaN values
df = df.dropna()

# -------------------------------------------------------------
# 2. Separate Features and Target
# -------------------------------------------------------------
X = df.drop("diagnosis", axis=1)   # all features
y = df["diagnosis"]                # target: M = malignant, B = benign

# -------------------------------------------------------------
# 3. Split Training & Testing Data
# -------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -------------------------------------------------------------
# 4. Standardize Features
# -------------------------------------------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# -------------------------------------------------------------
# 5. Apply PCA
# -------------------------------------------------------------
pca = PCA(n_components=2)   # Reduce to 2 dimensions for visualization
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print("Explained Variance Ratio:", pca.explained_variance_ratio_)

# -------------------------------------------------------------
# 6. Build Classification Model
# -------------------------------------------------------------
model = LogisticRegression()
model.fit(X_train_pca, y_train)

# -------------------------------------------------------------
# 7. Predictions
# -------------------------------------------------------------
y_pred = model.predict(X_test_pca)

# -------------------------------------------------------------
# 8. Evaluation
# -------------------------------------------------------------
print("\nAccuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# -------------------------------------------------------------
# 9. Visualization of PCA Components
# -------------------------------------------------------------
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=(y_train == 'M'), cmap='coolwarm')
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA Visualization of Breast Cancer Data")
plt.show()

"""# New Section"""

from google.colab import drive
drive.mount('/content/drive')



